**ğŸ¤– Offline AI Chatbot (Ollama + FastAPI)**

An offline, full-stack AI chatbot built using local Large Language Models (LLMs) powered by Ollama.
This project provides a ChatGPT-like experience with a modern UI, streaming responses, dark/light mode, and model switching â€” all without internet access.

**âœ¨ Features**

1.ğŸ”’ Fully Offline AI Chatbot

2.ğŸ¤– Uses local LLMs via Ollama

3.ğŸ” Model switching (LLaMA 3.1 / Qwen)

4.ğŸ’¬ Streaming typing effect (ChatGPT-like)

5.ğŸŒ™ Dark / Light mode toggle

6.âŒ¨ï¸ Keyboard support (Press Enter to send)

7.ğŸ¨ Clean, modern, centered UI

8.âš¡ FastAPI-based backend

**ğŸ› ï¸ Tech Stack**

1.Backend

Python

FastAPI

Ollama (Local LLM runtime)

2.Frontend

HTML

CSS

JavaScript

**ğŸ—ï¸ Project Architecture**

Frontend (HTML / CSS / JavaScript)
        â†“
FastAPI Backend (Python)
        â†“
Ollama Local LLM (Offline)

**ğŸ“‚ Project Structure**

offline-ai-chatbot/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ script.js
â”‚
â””â”€â”€ README.md

â–¶ï¸ How to Run the Project

1ï¸âƒ£ Install Ollama

Download and install Ollama from:
ğŸ‘‰ https://ollama.com

2ï¸âƒ£ Download a Model

Run once to download the model:

ollama run llama3.1:8b-instruct-q4_K_M


(Optional: You can also use qwen2.5:7b)

3ï¸âƒ£ Install Backend Dependencies
cd backend
pip install -r requirements.txt

4ï¸âƒ£ Start the Backend Server
python -m uvicorn main:app --reload


Backend runs at:

http://127.0.0.1:8000

5ï¸âƒ£ Run the Frontend

Open frontend/index.html
OR

Use VS Code Live Server


**ğŸ¯ Use Cases**

1.Offline AI assistant

2.Privacy-focused chatbot

3.Learning local LLM deployment

4.Full-stack AI project for resume & interviews

**ğŸ§‘â€ğŸ’» Author**

Diggum Yaswanth Kumar (Yashu)
B.E Graduate | AI & Full-Stack Enthusiast

GitHub: https://github.com/diggum-yaswanth-kumar
